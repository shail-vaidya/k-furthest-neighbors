{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "\n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [30, 70]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d29654c-aa0e-4166-90e1-40493275a5b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "QuantConv2d(\n",
      "  256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU(inplace=True)\n",
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n",
      "ReLU(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.features[23])\n",
    "print(model.features[24])\n",
    "print(model.features[25])\n",
    "print(model.features[26])\n",
    "print(model.features[27])\n",
    "\n",
    "print(model.features[28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 1.042 (1.042)\tData 0.999 (0.999)\tLoss 2.4765 (2.4765)\tPrec 10.156% (10.156%)\n",
      "Epoch: [0][100/391]\tTime 0.038 (0.051)\tData 0.002 (0.012)\tLoss 1.6814 (1.8940)\tPrec 41.406% (29.339%)\n",
      "Epoch: [0][200/391]\tTime 0.036 (0.046)\tData 0.003 (0.007)\tLoss 1.5946 (1.7513)\tPrec 43.750% (34.853%)\n",
      "Epoch: [0][300/391]\tTime 0.041 (0.045)\tData 0.003 (0.006)\tLoss 1.3900 (1.6728)\tPrec 46.875% (37.749%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.516 (0.516)\tLoss 1.3822 (1.3822)\tPrec 46.875% (46.875%)\n",
      " * Prec 49.010% \n",
      "best acc: 49.010000\n",
      "Epoch: [1][0/391]\tTime 1.050 (1.050)\tData 0.987 (0.987)\tLoss 1.2585 (1.2585)\tPrec 56.250% (56.250%)\n",
      "Epoch: [1][100/391]\tTime 0.036 (0.052)\tData 0.002 (0.012)\tLoss 1.3005 (1.3809)\tPrec 49.219% (49.373%)\n",
      "Epoch: [1][200/391]\tTime 0.043 (0.047)\tData 0.002 (0.007)\tLoss 1.1016 (1.3547)\tPrec 61.719% (50.641%)\n",
      "Epoch: [1][300/391]\tTime 0.043 (0.045)\tData 0.002 (0.006)\tLoss 1.2513 (1.3273)\tPrec 55.469% (51.806%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.721 (0.721)\tLoss 1.2431 (1.2431)\tPrec 54.688% (54.688%)\n",
      " * Prec 57.870% \n",
      "best acc: 57.870000\n",
      "Epoch: [2][0/391]\tTime 1.193 (1.193)\tData 1.138 (1.138)\tLoss 1.4008 (1.4008)\tPrec 52.344% (52.344%)\n",
      "Epoch: [2][100/391]\tTime 0.045 (0.052)\tData 0.003 (0.014)\tLoss 1.2546 (1.2004)\tPrec 51.562% (56.668%)\n",
      "Epoch: [2][200/391]\tTime 0.042 (0.046)\tData 0.002 (0.008)\tLoss 1.1023 (1.1756)\tPrec 62.500% (57.626%)\n",
      "Epoch: [2][300/391]\tTime 0.045 (0.045)\tData 0.003 (0.006)\tLoss 1.1257 (1.1589)\tPrec 60.156% (58.207%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.523 (0.523)\tLoss 0.9888 (0.9888)\tPrec 66.406% (66.406%)\n",
      " * Prec 61.220% \n",
      "best acc: 61.220000\n",
      "Epoch: [3][0/391]\tTime 1.000 (1.000)\tData 0.948 (0.948)\tLoss 0.9274 (0.9274)\tPrec 65.625% (65.625%)\n",
      "Epoch: [3][100/391]\tTime 0.035 (0.050)\tData 0.002 (0.012)\tLoss 0.9790 (1.0537)\tPrec 69.531% (61.897%)\n",
      "Epoch: [3][200/391]\tTime 0.050 (0.045)\tData 0.003 (0.007)\tLoss 0.9329 (1.0560)\tPrec 70.312% (61.886%)\n",
      "Epoch: [3][300/391]\tTime 0.042 (0.044)\tData 0.002 (0.006)\tLoss 0.9648 (1.0433)\tPrec 65.625% (62.544%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.731 (0.731)\tLoss 0.9611 (0.9611)\tPrec 64.062% (64.062%)\n",
      " * Prec 65.640% \n",
      "best acc: 65.640000\n",
      "Epoch: [4][0/391]\tTime 1.064 (1.064)\tData 1.009 (1.009)\tLoss 0.8103 (0.8103)\tPrec 68.750% (68.750%)\n",
      "Epoch: [4][100/391]\tTime 0.040 (0.051)\tData 0.003 (0.012)\tLoss 0.8215 (0.9614)\tPrec 69.531% (66.375%)\n",
      "Epoch: [4][200/391]\tTime 0.040 (0.046)\tData 0.002 (0.007)\tLoss 0.8376 (0.9648)\tPrec 74.219% (65.804%)\n",
      "Epoch: [4][300/391]\tTime 0.036 (0.045)\tData 0.002 (0.006)\tLoss 0.8705 (0.9499)\tPrec 67.188% (66.282%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.555 (0.555)\tLoss 0.8450 (0.8450)\tPrec 68.750% (68.750%)\n",
      " * Prec 67.000% \n",
      "best acc: 67.000000\n",
      "Epoch: [5][0/391]\tTime 1.001 (1.001)\tData 0.946 (0.946)\tLoss 0.8581 (0.8581)\tPrec 71.875% (71.875%)\n",
      "Epoch: [5][100/391]\tTime 0.041 (0.051)\tData 0.002 (0.012)\tLoss 0.9045 (0.8980)\tPrec 66.406% (68.185%)\n",
      "Epoch: [5][200/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.9465 (0.9024)\tPrec 67.188% (67.895%)\n",
      "Epoch: [5][300/391]\tTime 0.042 (0.043)\tData 0.002 (0.006)\tLoss 0.8674 (0.9018)\tPrec 66.406% (67.974%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.660 (0.660)\tLoss 0.8547 (0.8547)\tPrec 68.750% (68.750%)\n",
      " * Prec 69.020% \n",
      "best acc: 69.020000\n",
      "Epoch: [6][0/391]\tTime 1.082 (1.082)\tData 1.036 (1.036)\tLoss 0.8145 (0.8145)\tPrec 71.094% (71.094%)\n",
      "Epoch: [6][100/391]\tTime 0.033 (0.050)\tData 0.001 (0.012)\tLoss 0.5882 (0.8464)\tPrec 81.250% (70.019%)\n",
      "Epoch: [6][200/391]\tTime 0.034 (0.045)\tData 0.002 (0.007)\tLoss 0.8218 (0.8362)\tPrec 77.344% (70.713%)\n",
      "Epoch: [6][300/391]\tTime 0.040 (0.044)\tData 0.002 (0.006)\tLoss 0.7613 (0.8298)\tPrec 71.875% (70.884%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.663 (0.663)\tLoss 0.7731 (0.7731)\tPrec 73.438% (73.438%)\n",
      " * Prec 70.530% \n",
      "best acc: 70.530000\n",
      "Epoch: [7][0/391]\tTime 0.870 (0.870)\tData 0.820 (0.820)\tLoss 0.7560 (0.7560)\tPrec 73.438% (73.438%)\n",
      "Epoch: [7][100/391]\tTime 0.041 (0.046)\tData 0.002 (0.010)\tLoss 0.6709 (0.7952)\tPrec 74.219% (72.324%)\n",
      "Epoch: [7][200/391]\tTime 0.036 (0.044)\tData 0.002 (0.006)\tLoss 0.7569 (0.8011)\tPrec 71.875% (71.922%)\n",
      "Epoch: [7][300/391]\tTime 0.041 (0.043)\tData 0.002 (0.005)\tLoss 0.9616 (0.7924)\tPrec 67.969% (72.124%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.735 (0.735)\tLoss 0.6742 (0.6742)\tPrec 76.562% (76.562%)\n",
      " * Prec 73.160% \n",
      "best acc: 73.160000\n",
      "Epoch: [8][0/391]\tTime 0.998 (0.998)\tData 0.942 (0.942)\tLoss 0.7368 (0.7368)\tPrec 76.562% (76.562%)\n",
      "Epoch: [8][100/391]\tTime 0.040 (0.050)\tData 0.003 (0.012)\tLoss 0.7261 (0.7313)\tPrec 77.344% (74.126%)\n",
      "Epoch: [8][200/391]\tTime 0.043 (0.046)\tData 0.002 (0.007)\tLoss 0.7866 (0.7385)\tPrec 72.656% (74.017%)\n",
      "Epoch: [8][300/391]\tTime 0.042 (0.044)\tData 0.002 (0.005)\tLoss 0.7993 (0.7424)\tPrec 70.312% (73.739%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.463 (0.463)\tLoss 0.7728 (0.7728)\tPrec 71.875% (71.875%)\n",
      " * Prec 69.780% \n",
      "best acc: 73.160000\n",
      "Epoch: [9][0/391]\tTime 1.309 (1.309)\tData 1.254 (1.254)\tLoss 0.6910 (0.6910)\tPrec 74.219% (74.219%)\n",
      "Epoch: [9][100/391]\tTime 0.041 (0.053)\tData 0.002 (0.015)\tLoss 0.7059 (0.7280)\tPrec 73.438% (74.559%)\n",
      "Epoch: [9][200/391]\tTime 0.036 (0.047)\tData 0.002 (0.008)\tLoss 0.6010 (0.7149)\tPrec 78.906% (75.047%)\n",
      "Epoch: [9][300/391]\tTime 0.032 (0.045)\tData 0.002 (0.006)\tLoss 1.0455 (0.7131)\tPrec 61.719% (75.039%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.634 (0.634)\tLoss 0.7431 (0.7431)\tPrec 75.781% (75.781%)\n",
      " * Prec 70.600% \n",
      "best acc: 73.160000\n",
      "Epoch: [10][0/391]\tTime 1.562 (1.562)\tData 1.502 (1.502)\tLoss 0.5598 (0.5598)\tPrec 80.469% (80.469%)\n",
      "Epoch: [10][100/391]\tTime 0.043 (0.055)\tData 0.002 (0.017)\tLoss 0.6076 (0.6806)\tPrec 82.031% (76.269%)\n",
      "Epoch: [10][200/391]\tTime 0.040 (0.047)\tData 0.002 (0.010)\tLoss 0.7703 (0.6893)\tPrec 73.438% (75.995%)\n",
      "Epoch: [10][300/391]\tTime 0.030 (0.044)\tData 0.001 (0.007)\tLoss 0.4883 (0.6810)\tPrec 81.250% (76.222%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.905 (0.905)\tLoss 0.6381 (0.6381)\tPrec 78.125% (78.125%)\n",
      " * Prec 73.650% \n",
      "best acc: 73.650000\n",
      "Epoch: [11][0/391]\tTime 1.166 (1.166)\tData 1.113 (1.113)\tLoss 0.6758 (0.6758)\tPrec 78.125% (78.125%)\n",
      "Epoch: [11][100/391]\tTime 0.036 (0.048)\tData 0.002 (0.013)\tLoss 0.6571 (0.6427)\tPrec 75.000% (77.406%)\n",
      "Epoch: [11][200/391]\tTime 0.040 (0.043)\tData 0.002 (0.008)\tLoss 0.6569 (0.6561)\tPrec 77.344% (77.006%)\n",
      "Epoch: [11][300/391]\tTime 0.042 (0.041)\tData 0.002 (0.006)\tLoss 0.6140 (0.6562)\tPrec 72.656% (77.048%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.596 (0.596)\tLoss 0.6603 (0.6603)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.800% \n",
      "best acc: 76.800000\n",
      "Epoch: [12][0/391]\tTime 1.000 (1.000)\tData 0.945 (0.945)\tLoss 0.6266 (0.6266)\tPrec 81.250% (81.250%)\n",
      "Epoch: [12][100/391]\tTime 0.039 (0.048)\tData 0.002 (0.011)\tLoss 0.7459 (0.6249)\tPrec 73.438% (78.666%)\n",
      "Epoch: [12][200/391]\tTime 0.045 (0.042)\tData 0.002 (0.007)\tLoss 0.4923 (0.6239)\tPrec 88.281% (78.486%)\n",
      "Epoch: [12][300/391]\tTime 0.032 (0.041)\tData 0.002 (0.005)\tLoss 0.5097 (0.6269)\tPrec 82.031% (78.335%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.706 (0.706)\tLoss 0.5551 (0.5551)\tPrec 81.250% (81.250%)\n",
      " * Prec 76.860% \n",
      "best acc: 76.860000\n",
      "Epoch: [13][0/391]\tTime 0.870 (0.870)\tData 0.826 (0.826)\tLoss 0.6092 (0.6092)\tPrec 76.562% (76.562%)\n",
      "Epoch: [13][100/391]\tTime 0.042 (0.046)\tData 0.002 (0.010)\tLoss 0.6236 (0.6005)\tPrec 78.125% (78.914%)\n",
      "Epoch: [13][200/391]\tTime 0.038 (0.043)\tData 0.002 (0.006)\tLoss 0.6882 (0.6105)\tPrec 73.438% (78.786%)\n",
      "Epoch: [13][300/391]\tTime 0.026 (0.041)\tData 0.002 (0.005)\tLoss 0.5613 (0.6088)\tPrec 77.344% (78.924%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.558 (0.558)\tLoss 0.6584 (0.6584)\tPrec 77.344% (77.344%)\n",
      " * Prec 74.940% \n",
      "best acc: 76.860000\n",
      "Epoch: [14][0/391]\tTime 1.184 (1.184)\tData 1.136 (1.136)\tLoss 0.5480 (0.5480)\tPrec 78.125% (78.125%)\n",
      "Epoch: [14][100/391]\tTime 0.032 (0.049)\tData 0.001 (0.013)\tLoss 0.6755 (0.5999)\tPrec 75.781% (78.922%)\n",
      "Epoch: [14][200/391]\tTime 0.032 (0.042)\tData 0.002 (0.008)\tLoss 0.7441 (0.5999)\tPrec 74.219% (78.840%)\n",
      "Epoch: [14][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.006)\tLoss 0.7132 (0.6021)\tPrec 82.812% (78.950%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.828 (0.828)\tLoss 0.5799 (0.5799)\tPrec 82.031% (82.031%)\n",
      " * Prec 77.520% \n",
      "best acc: 77.520000\n",
      "Epoch: [15][0/391]\tTime 1.178 (1.178)\tData 1.130 (1.130)\tLoss 0.6389 (0.6389)\tPrec 75.781% (75.781%)\n",
      "Epoch: [15][100/391]\tTime 0.041 (0.047)\tData 0.002 (0.013)\tLoss 0.4673 (0.5650)\tPrec 82.812% (80.422%)\n",
      "Epoch: [15][200/391]\tTime 0.033 (0.042)\tData 0.002 (0.008)\tLoss 0.4101 (0.5695)\tPrec 84.375% (80.127%)\n",
      "Epoch: [15][300/391]\tTime 0.047 (0.040)\tData 0.002 (0.006)\tLoss 0.5985 (0.5711)\tPrec 79.688% (80.100%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.521 (0.521)\tLoss 0.5930 (0.5930)\tPrec 79.688% (79.688%)\n",
      " * Prec 78.730% \n",
      "best acc: 78.730000\n",
      "Epoch: [16][0/391]\tTime 0.807 (0.807)\tData 0.753 (0.753)\tLoss 0.4413 (0.4413)\tPrec 88.281% (88.281%)\n",
      "Epoch: [16][100/391]\tTime 0.041 (0.048)\tData 0.002 (0.010)\tLoss 0.5476 (0.5354)\tPrec 79.688% (81.196%)\n",
      "Epoch: [16][200/391]\tTime 0.035 (0.045)\tData 0.002 (0.006)\tLoss 0.5950 (0.5561)\tPrec 79.688% (80.527%)\n",
      "Epoch: [16][300/391]\tTime 0.043 (0.043)\tData 0.002 (0.005)\tLoss 0.4127 (0.5545)\tPrec 87.500% (80.669%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.603 (0.603)\tLoss 0.6195 (0.6195)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.980% \n",
      "best acc: 78.730000\n",
      "Epoch: [17][0/391]\tTime 1.146 (1.146)\tData 1.103 (1.103)\tLoss 0.6469 (0.6469)\tPrec 78.125% (78.125%)\n",
      "Epoch: [17][100/391]\tTime 0.044 (0.052)\tData 0.002 (0.013)\tLoss 0.5248 (0.5326)\tPrec 84.375% (81.513%)\n",
      "Epoch: [17][200/391]\tTime 0.050 (0.046)\tData 0.003 (0.008)\tLoss 0.6358 (0.5429)\tPrec 78.125% (81.028%)\n",
      "Epoch: [17][300/391]\tTime 0.035 (0.043)\tData 0.002 (0.006)\tLoss 0.4884 (0.5398)\tPrec 79.688% (81.081%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.566 (0.566)\tLoss 0.6163 (0.6163)\tPrec 73.438% (73.438%)\n",
      " * Prec 78.650% \n",
      "best acc: 78.730000\n",
      "Epoch: [18][0/391]\tTime 0.865 (0.865)\tData 0.806 (0.806)\tLoss 0.4610 (0.4610)\tPrec 82.812% (82.812%)\n",
      "Epoch: [18][100/391]\tTime 0.042 (0.042)\tData 0.002 (0.010)\tLoss 0.5659 (0.5328)\tPrec 82.031% (81.621%)\n",
      "Epoch: [18][200/391]\tTime 0.040 (0.040)\tData 0.002 (0.006)\tLoss 0.4959 (0.5233)\tPrec 80.469% (81.841%)\n",
      "Epoch: [18][300/391]\tTime 0.049 (0.040)\tData 0.003 (0.005)\tLoss 0.6193 (0.5251)\tPrec 79.688% (81.881%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.803 (0.803)\tLoss 0.5875 (0.5875)\tPrec 83.594% (83.594%)\n",
      " * Prec 79.160% \n",
      "best acc: 79.160000\n",
      "Epoch: [19][0/391]\tTime 0.880 (0.880)\tData 0.823 (0.823)\tLoss 0.3999 (0.3999)\tPrec 85.156% (85.156%)\n",
      "Epoch: [19][100/391]\tTime 0.036 (0.043)\tData 0.002 (0.010)\tLoss 0.5212 (0.5074)\tPrec 78.906% (82.480%)\n",
      "Epoch: [19][200/391]\tTime 0.035 (0.041)\tData 0.002 (0.006)\tLoss 0.6357 (0.5035)\tPrec 79.688% (82.603%)\n",
      "Epoch: [19][300/391]\tTime 0.035 (0.041)\tData 0.001 (0.005)\tLoss 0.3953 (0.5016)\tPrec 85.938% (82.745%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.747 (0.747)\tLoss 0.4727 (0.4727)\tPrec 85.156% (85.156%)\n",
      " * Prec 80.300% \n",
      "best acc: 80.300000\n",
      "Epoch: [20][0/391]\tTime 0.785 (0.785)\tData 0.744 (0.744)\tLoss 0.4328 (0.4328)\tPrec 83.594% (83.594%)\n",
      "Epoch: [20][100/391]\tTime 0.040 (0.049)\tData 0.002 (0.010)\tLoss 0.4726 (0.4842)\tPrec 83.594% (83.052%)\n",
      "Epoch: [20][200/391]\tTime 0.043 (0.046)\tData 0.003 (0.006)\tLoss 0.3569 (0.4875)\tPrec 89.844% (83.135%)\n",
      "Epoch: [20][300/391]\tTime 0.034 (0.044)\tData 0.002 (0.005)\tLoss 0.4566 (0.4833)\tPrec 83.594% (83.212%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.637 (0.637)\tLoss 0.5667 (0.5667)\tPrec 82.031% (82.031%)\n",
      " * Prec 79.940% \n",
      "best acc: 80.300000\n",
      "Epoch: [21][0/391]\tTime 1.138 (1.138)\tData 1.091 (1.091)\tLoss 0.5005 (0.5005)\tPrec 81.250% (81.250%)\n",
      "Epoch: [21][100/391]\tTime 0.043 (0.051)\tData 0.002 (0.013)\tLoss 0.5406 (0.4714)\tPrec 83.594% (83.779%)\n",
      "Epoch: [21][200/391]\tTime 0.033 (0.046)\tData 0.002 (0.008)\tLoss 0.4991 (0.4833)\tPrec 83.594% (83.178%)\n",
      "Epoch: [21][300/391]\tTime 0.035 (0.044)\tData 0.002 (0.006)\tLoss 0.6091 (0.4823)\tPrec 78.125% (83.181%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.550 (0.550)\tLoss 0.5906 (0.5906)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.700% \n",
      "best acc: 80.300000\n",
      "Epoch: [22][0/391]\tTime 1.158 (1.158)\tData 1.106 (1.106)\tLoss 0.5179 (0.5179)\tPrec 80.469% (80.469%)\n",
      "Epoch: [22][100/391]\tTime 0.029 (0.044)\tData 0.002 (0.013)\tLoss 0.2959 (0.4571)\tPrec 91.406% (84.205%)\n",
      "Epoch: [22][200/391]\tTime 0.037 (0.042)\tData 0.002 (0.008)\tLoss 0.5800 (0.4618)\tPrec 78.125% (84.045%)\n",
      "Epoch: [22][300/391]\tTime 0.038 (0.042)\tData 0.003 (0.006)\tLoss 0.5003 (0.4620)\tPrec 80.469% (84.038%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.479 (0.479)\tLoss 0.4840 (0.4840)\tPrec 81.250% (81.250%)\n",
      " * Prec 78.920% \n",
      "best acc: 80.300000\n",
      "Epoch: [23][0/391]\tTime 0.862 (0.862)\tData 0.811 (0.811)\tLoss 0.4808 (0.4808)\tPrec 84.375% (84.375%)\n",
      "Epoch: [23][100/391]\tTime 0.040 (0.049)\tData 0.002 (0.010)\tLoss 0.4320 (0.4437)\tPrec 83.594% (84.793%)\n",
      "Epoch: [23][200/391]\tTime 0.039 (0.045)\tData 0.002 (0.006)\tLoss 0.4993 (0.4495)\tPrec 82.031% (84.515%)\n",
      "Epoch: [23][300/391]\tTime 0.036 (0.044)\tData 0.002 (0.005)\tLoss 0.5169 (0.4569)\tPrec 82.812% (84.227%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.440 (0.440)\tLoss 0.5699 (0.5699)\tPrec 82.812% (82.812%)\n",
      " * Prec 80.760% \n",
      "best acc: 80.760000\n",
      "Epoch: [24][0/391]\tTime 0.729 (0.729)\tData 0.672 (0.672)\tLoss 0.4084 (0.4084)\tPrec 85.938% (85.938%)\n",
      "Epoch: [24][100/391]\tTime 0.036 (0.048)\tData 0.002 (0.009)\tLoss 0.5863 (0.4294)\tPrec 82.031% (85.342%)\n",
      "Epoch: [24][200/391]\tTime 0.044 (0.045)\tData 0.002 (0.006)\tLoss 0.5878 (0.4416)\tPrec 80.469% (84.989%)\n",
      "Epoch: [24][300/391]\tTime 0.043 (0.043)\tData 0.002 (0.005)\tLoss 0.3724 (0.4402)\tPrec 83.594% (84.936%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.790 (0.790)\tLoss 0.4694 (0.4694)\tPrec 80.469% (80.469%)\n",
      " * Prec 81.130% \n",
      "best acc: 81.130000\n",
      "Epoch: [25][0/391]\tTime 0.952 (0.952)\tData 0.893 (0.893)\tLoss 0.4919 (0.4919)\tPrec 82.812% (82.812%)\n",
      "Epoch: [25][100/391]\tTime 0.053 (0.053)\tData 0.003 (0.012)\tLoss 0.4199 (0.4207)\tPrec 87.500% (85.512%)\n",
      "Epoch: [25][200/391]\tTime 0.029 (0.044)\tData 0.001 (0.007)\tLoss 0.5366 (0.4224)\tPrec 80.469% (85.471%)\n",
      "Epoch: [25][300/391]\tTime 0.037 (0.040)\tData 0.003 (0.005)\tLoss 0.3694 (0.4268)\tPrec 88.281% (85.369%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.682 (0.682)\tLoss 0.3683 (0.3683)\tPrec 85.156% (85.156%)\n",
      " * Prec 82.320% \n",
      "best acc: 82.320000\n",
      "Epoch: [26][0/391]\tTime 1.072 (1.072)\tData 1.029 (1.029)\tLoss 0.2716 (0.2716)\tPrec 92.188% (92.188%)\n",
      "Epoch: [26][100/391]\tTime 0.036 (0.045)\tData 0.001 (0.012)\tLoss 0.3463 (0.4074)\tPrec 85.156% (85.481%)\n",
      "Epoch: [26][200/391]\tTime 0.038 (0.040)\tData 0.002 (0.007)\tLoss 0.3836 (0.4115)\tPrec 87.500% (85.615%)\n",
      "Epoch: [26][300/391]\tTime 0.033 (0.038)\tData 0.002 (0.005)\tLoss 0.4879 (0.4160)\tPrec 82.812% (85.569%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.568 (0.568)\tLoss 0.4113 (0.4113)\tPrec 86.719% (86.719%)\n",
      " * Prec 80.920% \n",
      "best acc: 82.320000\n",
      "Epoch: [27][0/391]\tTime 1.299 (1.299)\tData 1.247 (1.247)\tLoss 0.2969 (0.2969)\tPrec 88.281% (88.281%)\n",
      "Epoch: [27][100/391]\tTime 0.035 (0.046)\tData 0.002 (0.014)\tLoss 0.3068 (0.4051)\tPrec 89.062% (85.845%)\n",
      "Epoch: [27][200/391]\tTime 0.033 (0.040)\tData 0.001 (0.008)\tLoss 0.3636 (0.4036)\tPrec 89.062% (86.046%)\n",
      "Epoch: [27][300/391]\tTime 0.030 (0.038)\tData 0.002 (0.006)\tLoss 0.3771 (0.4033)\tPrec 86.719% (86.062%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.940 (0.940)\tLoss 0.4950 (0.4950)\tPrec 82.031% (82.031%)\n",
      " * Prec 80.380% \n",
      "best acc: 82.320000\n",
      "Epoch: [28][0/391]\tTime 1.026 (1.026)\tData 0.989 (0.989)\tLoss 0.2594 (0.2594)\tPrec 92.188% (92.188%)\n",
      "Epoch: [28][100/391]\tTime 0.038 (0.044)\tData 0.002 (0.012)\tLoss 0.3718 (0.3831)\tPrec 89.062% (86.912%)\n",
      "Epoch: [28][200/391]\tTime 0.037 (0.040)\tData 0.001 (0.007)\tLoss 0.3156 (0.3909)\tPrec 89.062% (86.501%)\n",
      "Epoch: [28][300/391]\tTime 0.031 (0.038)\tData 0.001 (0.005)\tLoss 0.3972 (0.3944)\tPrec 82.812% (86.423%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.676 (0.676)\tLoss 0.5193 (0.5193)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.800% \n",
      "best acc: 82.320000\n",
      "Epoch: [29][0/391]\tTime 1.124 (1.124)\tData 1.067 (1.067)\tLoss 0.3231 (0.3231)\tPrec 89.062% (89.062%)\n",
      "Epoch: [29][100/391]\tTime 0.043 (0.052)\tData 0.003 (0.013)\tLoss 0.3712 (0.3832)\tPrec 87.500% (86.757%)\n",
      "Epoch: [29][200/391]\tTime 0.041 (0.045)\tData 0.002 (0.008)\tLoss 0.3078 (0.3902)\tPrec 88.281% (86.451%)\n",
      "Epoch: [29][300/391]\tTime 0.042 (0.042)\tData 0.003 (0.006)\tLoss 0.4216 (0.3902)\tPrec 85.938% (86.467%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.570 (0.570)\tLoss 0.4923 (0.4923)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.870% \n",
      "best acc: 82.320000\n",
      "Epoch: [30][0/391]\tTime 0.786 (0.786)\tData 0.733 (0.733)\tLoss 0.2682 (0.2682)\tPrec 91.406% (91.406%)\n",
      "Epoch: [30][100/391]\tTime 0.043 (0.047)\tData 0.002 (0.009)\tLoss 0.3985 (0.3736)\tPrec 89.062% (87.191%)\n",
      "Epoch: [30][200/391]\tTime 0.048 (0.044)\tData 0.003 (0.006)\tLoss 0.3597 (0.3842)\tPrec 88.281% (86.606%)\n",
      "Epoch: [30][300/391]\tTime 0.040 (0.043)\tData 0.003 (0.005)\tLoss 0.3059 (0.3871)\tPrec 88.281% (86.573%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.568 (0.568)\tLoss 0.5692 (0.5692)\tPrec 81.250% (81.250%)\n",
      " * Prec 81.790% \n",
      "best acc: 82.320000\n",
      "Epoch: [31][0/391]\tTime 0.927 (0.927)\tData 0.880 (0.880)\tLoss 0.3403 (0.3403)\tPrec 89.062% (89.062%)\n",
      "Epoch: [31][100/391]\tTime 0.043 (0.050)\tData 0.002 (0.011)\tLoss 0.3932 (0.3584)\tPrec 85.938% (87.925%)\n",
      "Epoch: [31][200/391]\tTime 0.043 (0.045)\tData 0.003 (0.007)\tLoss 0.3802 (0.3680)\tPrec 85.156% (87.586%)\n",
      "Epoch: [31][300/391]\tTime 0.028 (0.043)\tData 0.002 (0.005)\tLoss 0.3566 (0.3743)\tPrec 87.500% (87.279%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.651 (0.651)\tLoss 0.5089 (0.5089)\tPrec 85.156% (85.156%)\n",
      " * Prec 81.480% \n",
      "best acc: 82.320000\n",
      "Epoch: [32][0/391]\tTime 0.986 (0.986)\tData 0.947 (0.947)\tLoss 0.3784 (0.3784)\tPrec 89.062% (89.062%)\n",
      "Epoch: [32][100/391]\tTime 0.040 (0.049)\tData 0.002 (0.011)\tLoss 0.4318 (0.3513)\tPrec 86.719% (87.794%)\n",
      "Epoch: [32][200/391]\tTime 0.030 (0.044)\tData 0.002 (0.007)\tLoss 0.3865 (0.3627)\tPrec 86.719% (87.508%)\n",
      "Epoch: [32][300/391]\tTime 0.046 (0.043)\tData 0.003 (0.005)\tLoss 0.4627 (0.3615)\tPrec 83.594% (87.503%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.541 (0.541)\tLoss 0.4234 (0.4234)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.260% \n",
      "best acc: 82.320000\n",
      "Epoch: [33][0/391]\tTime 1.281 (1.281)\tData 1.209 (1.209)\tLoss 0.3932 (0.3932)\tPrec 86.719% (86.719%)\n",
      "Epoch: [33][100/391]\tTime 0.025 (0.053)\tData 0.001 (0.014)\tLoss 0.4525 (0.3538)\tPrec 85.156% (87.794%)\n",
      "Epoch: [33][200/391]\tTime 0.040 (0.046)\tData 0.002 (0.008)\tLoss 0.4680 (0.3600)\tPrec 84.375% (87.589%)\n",
      "Epoch: [33][300/391]\tTime 0.041 (0.044)\tData 0.003 (0.006)\tLoss 0.4692 (0.3602)\tPrec 78.906% (87.614%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.687 (0.687)\tLoss 0.5242 (0.5242)\tPrec 82.031% (82.031%)\n",
      " * Prec 81.380% \n",
      "best acc: 82.320000\n",
      "Epoch: [34][0/391]\tTime 0.948 (0.948)\tData 0.903 (0.903)\tLoss 0.2740 (0.2740)\tPrec 86.719% (86.719%)\n",
      "Epoch: [34][100/391]\tTime 0.043 (0.049)\tData 0.003 (0.011)\tLoss 0.2939 (0.3334)\tPrec 89.844% (88.714%)\n",
      "Epoch: [34][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.007)\tLoss 0.3747 (0.3404)\tPrec 86.719% (88.308%)\n",
      "Epoch: [34][300/391]\tTime 0.047 (0.043)\tData 0.003 (0.005)\tLoss 0.4047 (0.3448)\tPrec 82.031% (88.123%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.632 (0.632)\tLoss 0.3212 (0.3212)\tPrec 91.406% (91.406%)\n",
      " * Prec 83.480% \n",
      "best acc: 83.480000\n",
      "Epoch: [35][0/391]\tTime 0.951 (0.951)\tData 0.897 (0.897)\tLoss 0.2727 (0.2727)\tPrec 89.062% (89.062%)\n",
      "Epoch: [35][100/391]\tTime 0.034 (0.050)\tData 0.002 (0.011)\tLoss 0.2426 (0.3330)\tPrec 91.406% (88.567%)\n",
      "Epoch: [35][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.007)\tLoss 0.3803 (0.3403)\tPrec 89.062% (88.511%)\n",
      "Epoch: [35][300/391]\tTime 0.029 (0.043)\tData 0.001 (0.005)\tLoss 0.4826 (0.3386)\tPrec 84.375% (88.530%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.601 (0.601)\tLoss 0.5089 (0.5089)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.350% \n",
      "best acc: 83.480000\n",
      "Epoch: [36][0/391]\tTime 0.906 (0.906)\tData 0.854 (0.854)\tLoss 0.3101 (0.3101)\tPrec 89.844% (89.844%)\n",
      "Epoch: [36][100/391]\tTime 0.044 (0.050)\tData 0.003 (0.011)\tLoss 0.3256 (0.3249)\tPrec 89.062% (88.923%)\n",
      "Epoch: [36][200/391]\tTime 0.033 (0.045)\tData 0.002 (0.007)\tLoss 0.2952 (0.3242)\tPrec 89.844% (88.985%)\n",
      "Epoch: [36][300/391]\tTime 0.035 (0.044)\tData 0.002 (0.005)\tLoss 0.2036 (0.3263)\tPrec 95.312% (88.930%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.568 (0.568)\tLoss 0.4208 (0.4208)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.160% \n",
      "best acc: 83.480000\n",
      "Epoch: [37][0/391]\tTime 1.433 (1.433)\tData 1.380 (1.380)\tLoss 0.3184 (0.3184)\tPrec 90.625% (90.625%)\n",
      "Epoch: [37][100/391]\tTime 0.024 (0.052)\tData 0.002 (0.016)\tLoss 0.3121 (0.3157)\tPrec 88.281% (89.295%)\n",
      "Epoch: [37][200/391]\tTime 0.040 (0.043)\tData 0.001 (0.009)\tLoss 0.3772 (0.3241)\tPrec 87.500% (88.798%)\n",
      "Epoch: [37][300/391]\tTime 0.043 (0.041)\tData 0.003 (0.007)\tLoss 0.3846 (0.3245)\tPrec 87.500% (88.891%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.555 (0.555)\tLoss 0.3547 (0.3547)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.380% \n",
      "best acc: 83.480000\n",
      "Epoch: [38][0/391]\tTime 1.233 (1.233)\tData 1.176 (1.176)\tLoss 0.2602 (0.2602)\tPrec 89.062% (89.062%)\n",
      "Epoch: [38][100/391]\tTime 0.040 (0.052)\tData 0.002 (0.014)\tLoss 0.2368 (0.3121)\tPrec 93.750% (89.070%)\n",
      "Epoch: [38][200/391]\tTime 0.043 (0.046)\tData 0.003 (0.008)\tLoss 0.3141 (0.3240)\tPrec 90.625% (88.740%)\n",
      "Epoch: [38][300/391]\tTime 0.035 (0.045)\tData 0.002 (0.006)\tLoss 0.3803 (0.3267)\tPrec 89.062% (88.712%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.644 (0.644)\tLoss 0.4081 (0.4081)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.090% \n",
      "best acc: 84.090000\n",
      "Epoch: [39][0/391]\tTime 0.939 (0.939)\tData 0.889 (0.889)\tLoss 0.3587 (0.3587)\tPrec 87.500% (87.500%)\n",
      "Epoch: [39][100/391]\tTime 0.045 (0.050)\tData 0.002 (0.011)\tLoss 0.3947 (0.3093)\tPrec 88.281% (89.503%)\n",
      "Epoch: [39][200/391]\tTime 0.042 (0.045)\tData 0.002 (0.007)\tLoss 0.2792 (0.3136)\tPrec 92.969% (89.373%)\n",
      "Epoch: [39][300/391]\tTime 0.039 (0.044)\tData 0.003 (0.005)\tLoss 0.3914 (0.3150)\tPrec 85.156% (89.260%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.677 (0.677)\tLoss 0.3642 (0.3642)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.710% \n",
      "best acc: 84.710000\n",
      "Epoch: [40][0/391]\tTime 0.946 (0.946)\tData 0.894 (0.894)\tLoss 0.2959 (0.2959)\tPrec 90.625% (90.625%)\n",
      "Epoch: [40][100/391]\tTime 0.037 (0.048)\tData 0.002 (0.011)\tLoss 0.2449 (0.3040)\tPrec 90.625% (89.588%)\n",
      "Epoch: [40][200/391]\tTime 0.037 (0.045)\tData 0.002 (0.007)\tLoss 0.4231 (0.3070)\tPrec 85.938% (89.366%)\n",
      "Epoch: [40][300/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.2048 (0.3059)\tPrec 95.312% (89.488%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.690 (0.690)\tLoss 0.4633 (0.4633)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.760% \n",
      "best acc: 84.710000\n",
      "Epoch: [41][0/391]\tTime 1.102 (1.102)\tData 1.056 (1.056)\tLoss 0.3939 (0.3939)\tPrec 81.250% (81.250%)\n",
      "Epoch: [41][100/391]\tTime 0.046 (0.047)\tData 0.003 (0.013)\tLoss 0.2745 (0.3035)\tPrec 89.844% (89.573%)\n",
      "Epoch: [41][200/391]\tTime 0.042 (0.043)\tData 0.002 (0.007)\tLoss 0.3533 (0.3035)\tPrec 87.500% (89.638%)\n",
      "Epoch: [41][300/391]\tTime 0.038 (0.042)\tData 0.002 (0.006)\tLoss 0.2612 (0.3024)\tPrec 90.625% (89.670%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.536 (0.536)\tLoss 0.4421 (0.4421)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.080% \n",
      "best acc: 84.710000\n",
      "Epoch: [42][0/391]\tTime 0.942 (0.942)\tData 0.895 (0.895)\tLoss 0.2946 (0.2946)\tPrec 89.844% (89.844%)\n",
      "Epoch: [42][100/391]\tTime 0.043 (0.044)\tData 0.002 (0.011)\tLoss 0.3188 (0.2938)\tPrec 89.062% (89.906%)\n",
      "Epoch: [42][200/391]\tTime 0.034 (0.042)\tData 0.002 (0.007)\tLoss 0.2995 (0.2929)\tPrec 91.406% (89.995%)\n",
      "Epoch: [42][300/391]\tTime 0.039 (0.042)\tData 0.003 (0.005)\tLoss 0.3158 (0.2990)\tPrec 86.719% (89.852%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.592 (0.592)\tLoss 0.6138 (0.6138)\tPrec 82.031% (82.031%)\n",
      " * Prec 82.890% \n",
      "best acc: 84.710000\n",
      "Epoch: [43][0/391]\tTime 0.977 (0.977)\tData 0.930 (0.930)\tLoss 0.3213 (0.3213)\tPrec 86.719% (86.719%)\n",
      "Epoch: [43][100/391]\tTime 0.043 (0.049)\tData 0.002 (0.011)\tLoss 0.3805 (0.2887)\tPrec 88.281% (90.053%)\n",
      "Epoch: [43][200/391]\tTime 0.043 (0.045)\tData 0.002 (0.007)\tLoss 0.3735 (0.2841)\tPrec 85.938% (90.310%)\n",
      "Epoch: [43][300/391]\tTime 0.041 (0.044)\tData 0.002 (0.005)\tLoss 0.2842 (0.2870)\tPrec 91.406% (90.181%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.636 (0.636)\tLoss 0.3701 (0.3701)\tPrec 82.812% (82.812%)\n",
      " * Prec 82.420% \n",
      "best acc: 84.710000\n",
      "Epoch: [44][0/391]\tTime 1.484 (1.484)\tData 1.444 (1.444)\tLoss 0.1795 (0.1795)\tPrec 94.531% (94.531%)\n",
      "Epoch: [44][100/391]\tTime 0.043 (0.054)\tData 0.003 (0.017)\tLoss 0.2608 (0.2662)\tPrec 89.844% (90.826%)\n",
      "Epoch: [44][200/391]\tTime 0.037 (0.047)\tData 0.002 (0.010)\tLoss 0.2207 (0.2758)\tPrec 91.406% (90.396%)\n",
      "Epoch: [44][300/391]\tTime 0.040 (0.045)\tData 0.003 (0.007)\tLoss 0.3090 (0.2825)\tPrec 88.281% (90.119%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.537 (0.537)\tLoss 0.4976 (0.4976)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.450% \n",
      "best acc: 84.710000\n",
      "Epoch: [45][0/391]\tTime 0.961 (0.961)\tData 0.918 (0.918)\tLoss 0.2177 (0.2177)\tPrec 92.188% (92.188%)\n",
      "Epoch: [45][100/391]\tTime 0.038 (0.049)\tData 0.003 (0.011)\tLoss 0.2596 (0.2650)\tPrec 88.281% (90.996%)\n",
      "Epoch: [45][200/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.2133 (0.2753)\tPrec 92.188% (90.637%)\n",
      "Epoch: [45][300/391]\tTime 0.044 (0.043)\tData 0.003 (0.005)\tLoss 0.2848 (0.2830)\tPrec 89.062% (90.358%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.591 (0.591)\tLoss 0.4011 (0.4011)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.450% \n",
      "best acc: 84.710000\n",
      "Epoch: [46][0/391]\tTime 0.913 (0.913)\tData 0.857 (0.857)\tLoss 0.2315 (0.2315)\tPrec 92.969% (92.969%)\n",
      "Epoch: [46][100/391]\tTime 0.044 (0.050)\tData 0.003 (0.011)\tLoss 0.1890 (0.2622)\tPrec 94.531% (91.136%)\n",
      "Epoch: [46][200/391]\tTime 0.047 (0.046)\tData 0.003 (0.007)\tLoss 0.2814 (0.2719)\tPrec 90.625% (90.761%)\n",
      "Epoch: [46][300/391]\tTime 0.036 (0.044)\tData 0.002 (0.005)\tLoss 0.2624 (0.2734)\tPrec 91.406% (90.750%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.800 (0.800)\tLoss 0.4092 (0.4092)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.520% \n",
      "best acc: 84.710000\n",
      "Epoch: [47][0/391]\tTime 1.096 (1.096)\tData 1.048 (1.048)\tLoss 0.2092 (0.2092)\tPrec 92.969% (92.969%)\n",
      "Epoch: [47][100/391]\tTime 0.037 (0.046)\tData 0.002 (0.012)\tLoss 0.2582 (0.2638)\tPrec 90.625% (90.795%)\n",
      "Epoch: [47][200/391]\tTime 0.037 (0.043)\tData 0.002 (0.007)\tLoss 0.2229 (0.2662)\tPrec 91.406% (90.780%)\n",
      "Epoch: [47][300/391]\tTime 0.030 (0.041)\tData 0.002 (0.006)\tLoss 0.2285 (0.2656)\tPrec 92.969% (90.892%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.464 (0.464)\tLoss 0.4622 (0.4622)\tPrec 84.375% (84.375%)\n",
      " * Prec 85.680% \n",
      "best acc: 85.680000\n",
      "Epoch: [48][0/391]\tTime 1.115 (1.115)\tData 1.069 (1.069)\tLoss 0.3366 (0.3366)\tPrec 92.188% (92.188%)\n",
      "Epoch: [48][100/391]\tTime 0.031 (0.044)\tData 0.002 (0.012)\tLoss 0.2053 (0.2559)\tPrec 94.531% (91.375%)\n",
      "Epoch: [48][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.007)\tLoss 0.3375 (0.2571)\tPrec 87.500% (91.278%)\n",
      "Epoch: [48][300/391]\tTime 0.043 (0.041)\tData 0.003 (0.006)\tLoss 0.3469 (0.2595)\tPrec 84.375% (91.139%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.584 (0.584)\tLoss 0.4484 (0.4484)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.330% \n",
      "best acc: 85.680000\n",
      "Epoch: [49][0/391]\tTime 0.705 (0.705)\tData 0.628 (0.628)\tLoss 0.1872 (0.1872)\tPrec 94.531% (94.531%)\n",
      "Epoch: [49][100/391]\tTime 0.041 (0.047)\tData 0.002 (0.009)\tLoss 0.1821 (0.2578)\tPrec 93.750% (91.136%)\n",
      "Epoch: [49][200/391]\tTime 0.039 (0.041)\tData 0.002 (0.005)\tLoss 0.2168 (0.2610)\tPrec 92.188% (90.971%)\n",
      "Epoch: [49][300/391]\tTime 0.041 (0.041)\tData 0.002 (0.004)\tLoss 0.3242 (0.2607)\tPrec 89.062% (91.017%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.622 (0.622)\tLoss 0.2928 (0.2928)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.170% \n",
      "best acc: 85.680000\n",
      "Epoch: [50][0/391]\tTime 0.912 (0.912)\tData 0.864 (0.864)\tLoss 0.1836 (0.1836)\tPrec 92.969% (92.969%)\n",
      "Epoch: [50][100/391]\tTime 0.042 (0.045)\tData 0.002 (0.011)\tLoss 0.3598 (0.2435)\tPrec 89.062% (91.863%)\n",
      "Epoch: [50][200/391]\tTime 0.042 (0.040)\tData 0.002 (0.006)\tLoss 0.3198 (0.2449)\tPrec 92.188% (91.624%)\n",
      "Epoch: [50][300/391]\tTime 0.036 (0.039)\tData 0.002 (0.005)\tLoss 0.2587 (0.2540)\tPrec 91.406% (91.328%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.760 (0.760)\tLoss 0.3502 (0.3502)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.660% \n",
      "best acc: 85.680000\n",
      "Epoch: [51][0/391]\tTime 1.024 (1.024)\tData 0.973 (0.973)\tLoss 0.1927 (0.1927)\tPrec 93.750% (93.750%)\n",
      "Epoch: [51][100/391]\tTime 0.030 (0.044)\tData 0.002 (0.012)\tLoss 0.4358 (0.2381)\tPrec 89.062% (91.894%)\n",
      "Epoch: [51][200/391]\tTime 0.035 (0.039)\tData 0.002 (0.007)\tLoss 0.3460 (0.2475)\tPrec 89.844% (91.546%)\n",
      "Epoch: [51][300/391]\tTime 0.049 (0.039)\tData 0.003 (0.005)\tLoss 0.3254 (0.2481)\tPrec 88.281% (91.585%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.585 (0.585)\tLoss 0.4226 (0.4226)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.010% \n",
      "best acc: 85.680000\n",
      "Epoch: [52][0/391]\tTime 0.956 (0.956)\tData 0.919 (0.919)\tLoss 0.2654 (0.2654)\tPrec 92.188% (92.188%)\n",
      "Epoch: [52][100/391]\tTime 0.036 (0.046)\tData 0.002 (0.011)\tLoss 0.3299 (0.2256)\tPrec 86.719% (92.474%)\n",
      "Epoch: [52][200/391]\tTime 0.040 (0.042)\tData 0.002 (0.007)\tLoss 0.3148 (0.2332)\tPrec 89.062% (92.156%)\n",
      "Epoch: [52][300/391]\tTime 0.041 (0.040)\tData 0.002 (0.005)\tLoss 0.2648 (0.2403)\tPrec 89.844% (91.824%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.667 (0.667)\tLoss 0.3535 (0.3535)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.190% \n",
      "best acc: 85.680000\n",
      "Epoch: [53][0/391]\tTime 1.007 (1.007)\tData 0.959 (0.959)\tLoss 0.1367 (0.1367)\tPrec 96.094% (96.094%)\n",
      "Epoch: [53][100/391]\tTime 0.034 (0.045)\tData 0.002 (0.011)\tLoss 0.2321 (0.2329)\tPrec 92.969% (92.010%)\n",
      "Epoch: [53][200/391]\tTime 0.036 (0.041)\tData 0.001 (0.007)\tLoss 0.3700 (0.2379)\tPrec 87.500% (91.923%)\n",
      "Epoch: [53][300/391]\tTime 0.032 (0.039)\tData 0.001 (0.005)\tLoss 0.1802 (0.2410)\tPrec 92.969% (91.738%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.593 (0.593)\tLoss 0.3248 (0.3248)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.490% \n",
      "best acc: 85.680000\n",
      "Epoch: [54][0/391]\tTime 0.854 (0.854)\tData 0.800 (0.800)\tLoss 0.1993 (0.1993)\tPrec 93.750% (93.750%)\n",
      "Epoch: [54][100/391]\tTime 0.041 (0.044)\tData 0.002 (0.010)\tLoss 0.2138 (0.2374)\tPrec 93.750% (91.979%)\n",
      "Epoch: [54][200/391]\tTime 0.031 (0.038)\tData 0.001 (0.006)\tLoss 0.2852 (0.2352)\tPrec 87.500% (92.009%)\n",
      "Epoch: [54][300/391]\tTime 0.024 (0.036)\tData 0.002 (0.004)\tLoss 0.2346 (0.2376)\tPrec 91.406% (91.954%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.680 (0.680)\tLoss 0.3486 (0.3486)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.430% \n",
      "best acc: 85.680000\n",
      "Epoch: [55][0/391]\tTime 1.138 (1.138)\tData 1.065 (1.065)\tLoss 0.3466 (0.3466)\tPrec 89.062% (89.062%)\n",
      "Epoch: [55][100/391]\tTime 0.043 (0.052)\tData 0.002 (0.013)\tLoss 0.2762 (0.2348)\tPrec 92.188% (92.133%)\n",
      "Epoch: [55][200/391]\tTime 0.037 (0.047)\tData 0.002 (0.008)\tLoss 0.2727 (0.2362)\tPrec 91.406% (92.048%)\n",
      "Epoch: [55][300/391]\tTime 0.038 (0.045)\tData 0.002 (0.006)\tLoss 0.2165 (0.2348)\tPrec 93.750% (92.047%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.658 (0.658)\tLoss 0.3308 (0.3308)\tPrec 86.719% (86.719%)\n",
      " * Prec 86.420% \n",
      "best acc: 86.420000\n",
      "Epoch: [56][0/391]\tTime 0.629 (0.629)\tData 0.588 (0.588)\tLoss 0.1897 (0.1897)\tPrec 94.531% (94.531%)\n",
      "Epoch: [56][100/391]\tTime 0.039 (0.047)\tData 0.002 (0.008)\tLoss 0.2081 (0.2253)\tPrec 93.750% (92.234%)\n",
      "Epoch: [56][200/391]\tTime 0.045 (0.044)\tData 0.003 (0.006)\tLoss 0.1415 (0.2282)\tPrec 96.094% (92.215%)\n",
      "Epoch: [56][300/391]\tTime 0.037 (0.043)\tData 0.002 (0.005)\tLoss 0.2718 (0.2313)\tPrec 89.844% (92.164%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.677 (0.677)\tLoss 0.2832 (0.2832)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.920% \n",
      "best acc: 86.420000\n",
      "Epoch: [57][0/391]\tTime 1.264 (1.264)\tData 1.209 (1.209)\tLoss 0.1994 (0.1994)\tPrec 92.969% (92.969%)\n",
      "Epoch: [57][100/391]\tTime 0.039 (0.054)\tData 0.002 (0.015)\tLoss 0.1906 (0.2114)\tPrec 95.312% (92.845%)\n",
      "Epoch: [57][200/391]\tTime 0.042 (0.047)\tData 0.002 (0.009)\tLoss 0.3308 (0.2205)\tPrec 92.188% (92.386%)\n",
      "Epoch: [57][300/391]\tTime 0.043 (0.045)\tData 0.002 (0.007)\tLoss 0.2005 (0.2290)\tPrec 92.969% (92.078%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.659 (0.659)\tLoss 0.3296 (0.3296)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.920% \n",
      "best acc: 86.420000\n",
      "Epoch: [58][0/391]\tTime 0.685 (0.685)\tData 0.629 (0.629)\tLoss 0.2568 (0.2568)\tPrec 88.281% (88.281%)\n",
      "Epoch: [58][100/391]\tTime 0.041 (0.043)\tData 0.002 (0.008)\tLoss 0.2121 (0.2069)\tPrec 91.406% (93.077%)\n",
      "Epoch: [58][200/391]\tTime 0.037 (0.041)\tData 0.002 (0.005)\tLoss 0.2433 (0.2121)\tPrec 91.406% (92.868%)\n",
      "Epoch: [58][300/391]\tTime 0.045 (0.041)\tData 0.002 (0.004)\tLoss 0.2610 (0.2163)\tPrec 91.406% (92.730%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.460 (0.460)\tLoss 0.4347 (0.4347)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.840% \n",
      "best acc: 86.420000\n",
      "Epoch: [59][0/391]\tTime 0.884 (0.884)\tData 0.830 (0.830)\tLoss 0.2399 (0.2399)\tPrec 91.406% (91.406%)\n",
      "Epoch: [59][100/391]\tTime 0.040 (0.048)\tData 0.002 (0.010)\tLoss 0.3206 (0.2194)\tPrec 88.281% (92.559%)\n",
      "Epoch: [59][200/391]\tTime 0.041 (0.045)\tData 0.003 (0.006)\tLoss 0.1954 (0.2161)\tPrec 94.531% (92.704%)\n",
      "Epoch: [59][300/391]\tTime 0.045 (0.043)\tData 0.002 (0.005)\tLoss 0.1944 (0.2158)\tPrec 93.750% (92.738%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.545 (0.545)\tLoss 0.4079 (0.4079)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.260% \n",
      "best acc: 86.420000\n",
      "Epoch: [60][0/391]\tTime 1.085 (1.085)\tData 1.029 (1.029)\tLoss 0.1383 (0.1383)\tPrec 93.750% (93.750%)\n",
      "Epoch: [60][100/391]\tTime 0.034 (0.050)\tData 0.002 (0.012)\tLoss 0.1810 (0.2048)\tPrec 93.750% (93.154%)\n",
      "Epoch: [60][200/391]\tTime 0.041 (0.045)\tData 0.002 (0.007)\tLoss 0.3691 (0.2125)\tPrec 86.719% (92.806%)\n",
      "Epoch: [60][300/391]\tTime 0.042 (0.044)\tData 0.002 (0.006)\tLoss 0.2381 (0.2114)\tPrec 92.969% (92.761%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.545 (0.545)\tLoss 0.4952 (0.4952)\tPrec 79.688% (79.688%)\n",
      " * Prec 82.970% \n",
      "best acc: 86.420000\n",
      "Epoch: [61][0/391]\tTime 1.224 (1.224)\tData 1.180 (1.180)\tLoss 0.2888 (0.2888)\tPrec 91.406% (91.406%)\n",
      "Epoch: [61][100/391]\tTime 0.037 (0.052)\tData 0.002 (0.014)\tLoss 0.1596 (0.2046)\tPrec 92.969% (92.799%)\n",
      "Epoch: [61][200/391]\tTime 0.038 (0.046)\tData 0.002 (0.008)\tLoss 0.1920 (0.2022)\tPrec 95.312% (93.004%)\n",
      "Epoch: [61][300/391]\tTime 0.036 (0.045)\tData 0.002 (0.006)\tLoss 0.1525 (0.2041)\tPrec 95.312% (93.008%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.573 (0.573)\tLoss 0.4071 (0.4071)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.370% \n",
      "best acc: 86.420000\n",
      "Epoch: [62][0/391]\tTime 1.009 (1.009)\tData 0.955 (0.955)\tLoss 0.2053 (0.2053)\tPrec 93.750% (93.750%)\n",
      "Epoch: [62][100/391]\tTime 0.043 (0.049)\tData 0.003 (0.012)\tLoss 0.2070 (0.2038)\tPrec 90.625% (93.108%)\n",
      "Epoch: [62][200/391]\tTime 0.038 (0.045)\tData 0.003 (0.007)\tLoss 0.2137 (0.2021)\tPrec 92.969% (93.171%)\n",
      "Epoch: [62][300/391]\tTime 0.039 (0.043)\tData 0.002 (0.005)\tLoss 0.2655 (0.2062)\tPrec 90.625% (92.995%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.657 (0.657)\tLoss 0.2832 (0.2832)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.820% \n",
      "best acc: 86.420000\n",
      "Epoch: [63][0/391]\tTime 1.474 (1.474)\tData 1.423 (1.423)\tLoss 0.2663 (0.2663)\tPrec 90.625% (90.625%)\n",
      "Epoch: [63][100/391]\tTime 0.038 (0.054)\tData 0.002 (0.016)\tLoss 0.2081 (0.1975)\tPrec 92.969% (93.464%)\n",
      "Epoch: [63][200/391]\tTime 0.040 (0.047)\tData 0.002 (0.009)\tLoss 0.2523 (0.1999)\tPrec 92.969% (93.299%)\n",
      "Epoch: [63][300/391]\tTime 0.039 (0.045)\tData 0.002 (0.007)\tLoss 0.1954 (0.2076)\tPrec 92.188% (93.031%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.653 (0.653)\tLoss 0.3378 (0.3378)\tPrec 87.500% (87.500%)\n",
      " * Prec 86.100% \n",
      "best acc: 86.420000\n",
      "Epoch: [64][0/391]\tTime 1.004 (1.004)\tData 0.948 (0.948)\tLoss 0.2523 (0.2523)\tPrec 92.969% (92.969%)\n",
      "Epoch: [64][100/391]\tTime 0.042 (0.050)\tData 0.002 (0.012)\tLoss 0.2214 (0.1969)\tPrec 91.406% (93.301%)\n",
      "Epoch: [64][200/391]\tTime 0.031 (0.044)\tData 0.002 (0.007)\tLoss 0.3466 (0.2023)\tPrec 89.844% (93.113%)\n",
      "Epoch: [64][300/391]\tTime 0.042 (0.042)\tData 0.002 (0.005)\tLoss 0.2875 (0.2056)\tPrec 92.188% (92.961%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.727 (0.727)\tLoss 0.2608 (0.2608)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.120% \n",
      "best acc: 86.420000\n",
      "Epoch: [65][0/391]\tTime 1.050 (1.050)\tData 0.994 (0.994)\tLoss 0.3077 (0.3077)\tPrec 90.625% (90.625%)\n",
      "Epoch: [65][100/391]\tTime 0.041 (0.050)\tData 0.002 (0.012)\tLoss 0.1845 (0.1911)\tPrec 89.844% (93.309%)\n",
      "Epoch: [65][200/391]\tTime 0.036 (0.046)\tData 0.002 (0.007)\tLoss 0.0919 (0.2013)\tPrec 97.656% (93.198%)\n",
      "Epoch: [65][300/391]\tTime 0.036 (0.045)\tData 0.002 (0.006)\tLoss 0.1911 (0.2014)\tPrec 94.531% (93.187%)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 6e-4\n",
    "weight_decay = 6e-3\n",
    "epochs = 300\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "fdir = 'result/'+'VGG_16_project'\n",
    "\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult/VGG16_quant/model_best.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_gpu \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1377\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1387\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:266\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 266\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:250\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    247\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    255\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "\n",
    "model()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i=0\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if ( i == 38):\n",
    "        print(layer)\n",
    "        layer.register_forward_pre_hook(save_output) \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "labels = labels.to(device) \n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd837b45-d903-4533-8a20-e8d59c2e01b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.features[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7., -7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7., -7.,  7.],\n",
      "          [ 7., -7.,  7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [ 7., -7.,  7.],\n",
      "          [ 7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7., -7., -7.],\n",
      "          [ 7., -7.,  7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.]]],\n",
      "\n",
      "\n",
      "        [[[-7.,  7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[-7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7., -7., -7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [ 7., -7., -7.],\n",
      "          [-7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7., -7.],\n",
      "          [-7.,  7., -7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.]]],\n",
      "\n",
      "\n",
      "        [[[-7., -7., -7.],\n",
      "          [-7.,  7., -7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [-7., -7., -7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[-7.,  7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7., -7.,  7.],\n",
      "          [ 7.,  7.,  7.]]],\n",
      "\n",
      "\n",
      "        [[[-7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[ 7., -7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[-7.,  7.,  7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7., -7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[-7.,  7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [ 7., -7., -7.]],\n",
      "\n",
      "         [[-7., -7.,  7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [ 7., -7.,  7.]],\n",
      "\n",
      "         [[-7.,  7., -7.],\n",
      "          [ 7., -7., -7.],\n",
      "          [-7., -7., -7.]]],\n",
      "\n",
      "\n",
      "        [[[ 7., -7., -7.],\n",
      "          [ 7., -7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[-7., -7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7., -7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7., -7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7., -7., -7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[-7.,  7., -7.],\n",
      "          [-7., -7., -7.],\n",
      "          [ 7.,  7., -7.]]],\n",
      "\n",
      "\n",
      "        [[[-7., -7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7.,  7.],\n",
      "          [ 7., -7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [ 7., -7.,  7.]],\n",
      "\n",
      "         [[-7., -7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[-7.,  7., -7.],\n",
      "          [-7., -7., -7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[-7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7., -7.,  7.]]],\n",
      "\n",
      "\n",
      "        [[[ 7., -7., -7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [ 7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7., -7.,  7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7.,  7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7.,  7., -7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [ 7., -7., -7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7.,  7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.]]],\n",
      "\n",
      "\n",
      "        [[[ 7.,  7., -7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [-7., -7.,  7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7., -7., -7.]],\n",
      "\n",
      "         [[ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7.,  7.],\n",
      "          [-7., -7.,  7.],\n",
      "          [-7.,  7., -7.]],\n",
      "\n",
      "         [[ 7., -7., -7.],\n",
      "          [ 7.,  7., -7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7., -7.,  7.],\n",
      "          [-7., -7., -7.],\n",
      "          [ 7.,  7.,  7.]],\n",
      "\n",
      "         [[ 7., -7., -7.],\n",
      "          [ 7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.]],\n",
      "\n",
      "         [[-7., -7., -7.],\n",
      "          [-7.,  7.,  7.],\n",
      "          [-7.,  7.,  7.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bits = 4\n",
    "weight_q = model.features[27].weight_q # quantized value is stored during the training\n",
    "w_alpha =  model.features[27].weight_quant.wgt_alpha  # alpha is defined in your model already. bring it out here\n",
    "w_delta =  w_alpha/(2**(w_bits-1)-1)  # delta can be calculated by using alpha and w_bit\n",
    "weight_int = weight_q/w_delta # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 4.0000, 11.0000,  9.0000,  4.0000],\n",
      "          [ 2.0000,  9.0000,  0.0000,  0.0000],\n",
      "          [ 6.0000, 15.0000,  8.0000,  0.0000],\n",
      "          [ 1.0000,  4.0000,  3.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  5.0000,  1.0000,  1.0000],\n",
      "          [ 3.0000,  4.0000,  5.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  3.0000,  7.0000],\n",
      "          [ 0.0000,  0.0000,  3.0000,  6.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0000,  3.0000,  3.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  0.0000],\n",
      "          [ 0.0000,  8.0000, 15.0000,  8.0000],\n",
      "          [ 0.0000,  2.0000,  6.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  4.0000,  2.0000,  0.0000],\n",
      "          [ 0.0000,  4.0000,  4.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  1.0000,  1.0000],\n",
      "          [ 0.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  3.0000,  4.0000,  4.0000],\n",
      "          [ 0.0000,  6.0000,  4.0000,  5.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  7.0000, 15.0000, 12.0000],\n",
      "          [ 2.0000, 10.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 2.0000,  2.0000,  2.0000,  6.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 3.0000,  9.0000, 12.0000,  8.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  5.0000,  5.0000,  2.0000],\n",
      "          [ 3.0000, 13.0000,  7.0000,  0.0000],\n",
      "          [ 8.0000, 15.0000, 13.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000, 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  4.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  2.0000,  0.0000],\n",
      "          [ 8.0000, 15.0000, 15.0000,  7.0000],\n",
      "          [11.0000, 15.0000, 15.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 6.0000, 12.0000,  7.0000,  3.0000],\n",
      "          [ 8.0000,  9.0000,  4.0000,  3.0000],\n",
      "          [ 8.0000,  3.0000,  1.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  6.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  6.0000,  4.0000,  0.0000],\n",
      "          [ 0.0000,  5.0000,  2.0000,  0.0000],\n",
      "          [ 1.0000,  5.0000,  5.0000,  1.0000]],\n",
      "\n",
      "         [[ 0.0000,  3.0000,  7.0000,  4.0000],\n",
      "          [ 2.0000, 12.0000,  7.0000,  5.0000],\n",
      "          [ 5.0000,  7.0000,  0.0000,  1.0000],\n",
      "          [ 4.0000,  4.0000,  1.0000,  1.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0000,  7.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  2.0000,  4.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  4.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  6.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  8.0000],\n",
      "          [ 0.0000,  1.0000,  2.0000,  6.0000]],\n",
      "\n",
      "         [[ 4.0000,  9.0000,  5.0000,  2.0000],\n",
      "          [ 4.0000,  9.0000,  4.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 1.0000,  1.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[13.0000, 15.0000, 11.0000,  3.0000],\n",
      "          [13.0000, 14.0000, 10.0000,  3.0000],\n",
      "          [ 5.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 2.0000,  1.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  9.0000,  6.0000,  0.0000],\n",
      "          [ 0.0000,  5.0000,  9.0000,  1.0000],\n",
      "          [ 0.0000,  1.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  2.0000,  4.0000,  4.0000]],\n",
      "\n",
      "         [[ 0.0000,  8.0000,  9.0000,  4.0000],\n",
      "          [ 7.0000, 12.0000,  7.0000,  7.0000],\n",
      "          [ 5.0000,  7.0000,  2.0000,  5.0000],\n",
      "          [ 0.0000,  2.0000,  3.0000,  4.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.0000, 10.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  4.0000,  5.0000,  0.0000],\n",
      "          [ 0.0000,  2.0000,  2.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  6.0000],\n",
      "          [ 0.0000,  5.0000,  0.0000,  5.0000],\n",
      "          [ 1.0000,  5.0000,  5.0000,  6.0000]],\n",
      "\n",
      "         [[10.0000, 15.0000,  7.0000,  1.0000],\n",
      "          [10.0000, 10.0000,  4.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  2.0000,  2.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 1.0000,  0.0000,  2.0000,  5.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 5.0000, 15.0000, 15.0000,  3.0000],\n",
      "          [ 3.0000, 15.0000, 15.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  1.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  2.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 4.0000, 12.0000,  6.0000,  4.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 5.0000,  5.0000,  0.0000,  0.0000],\n",
      "          [ 7.0000,  6.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  1.0000,  3.0000,  4.0000],\n",
      "          [ 0.0000,  0.0000,  4.0000,  6.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  4.0000],\n",
      "          [10.0000, 15.0000, 13.0000, 10.0000]],\n",
      "\n",
      "         [[ 2.0000,  3.0000,  2.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [12.0000, 15.0000,  6.0000,  4.0000],\n",
      "          [12.0000, 10.0000,  1.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 4.0000,  2.0000,  2.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  3.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 2.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 0.0000,  3.0000,  4.0000,  3.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  1.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  2.0000],\n",
      "          [ 1.0000,  4.0000,  4.0000,  5.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  2.0000,  2.0000,  0.0000],\n",
      "          [ 0.0000,  3.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  3.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  4.0000,  3.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  4.0000,  1.0000,  3.0000],\n",
      "          [ 5.0000, 15.0000,  8.0000,  5.0000],\n",
      "          [ 4.0000, 15.0000, 10.0000,  8.0000],\n",
      "          [ 5.0000, 10.0000,  9.0000,  7.0000]],\n",
      "\n",
      "         [[ 0.0000,  2.0000,  0.0000,  0.0000],\n",
      "          [ 9.0000,  5.0000,  4.0000,  0.0000],\n",
      "          [ 5.0000,  2.0000,  1.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bits = 4    \n",
    "x = save_output.outputs[0][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.features[27].act_alpha.item()\n",
    "x_delta = x_alpha/(2**(x_bits)-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.0969e-02,  1.0421e-01],\n",
      "          [ 5.4845e-02, -5.4845e-02]],\n",
      "\n",
      "         [[-8.7752e-02,  1.3711e-01],\n",
      "          [ 1.0969e-02,  8.7752e-02]],\n",
      "\n",
      "         [[-9.8721e-02, -2.3583e-01],\n",
      "          [-2.1938e-02, -8.7752e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.9616e-01,  4.5521e-01],\n",
      "          [ 2.5229e-01,  5.3748e-01]],\n",
      "\n",
      "         [[-3.2907e-02, -2.5777e-01],\n",
      "          [ 1.0969e-01,  7.6783e-02]],\n",
      "\n",
      "         [[-1.0969e-02,  2.7422e-02],\n",
      "          [-4.3876e-02, -2.6326e-01]]],\n",
      "\n",
      "\n",
      "        [[[-4.3876e-02, -1.0969e-02],\n",
      "          [-4.3876e-02,  2.8519e-01]],\n",
      "\n",
      "         [[ 3.2907e-02,  2.3035e-01],\n",
      "          [-3.8391e-01,  3.2907e-02]],\n",
      "\n",
      "         [[ 5.1554e-01,  1.2066e-01],\n",
      "          [ 3.7295e-01,  3.0713e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0585e-01,  2.4132e-01],\n",
      "          [ 4.6070e-01,  1.6453e-01]],\n",
      "\n",
      "         [[-2.1938e-01,  7.6783e-02],\n",
      "          [ 1.9744e-01,  5.5942e-01]],\n",
      "\n",
      "         [[-1.0969e-02, -1.3163e-01],\n",
      "          [ 3.5101e-01,  2.3035e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 3.7843e-01,  5.2103e-01],\n",
      "          [ 2.0841e-01,  2.7971e-01]],\n",
      "\n",
      "         [[ 2.7971e-01,  2.2486e-01],\n",
      "          [ 3.2907e-02,  4.9360e-02]],\n",
      "\n",
      "         [[-1.2614e-01, -3.8391e-02],\n",
      "          [ 2.1938e-02, -5.4845e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.2358e-01,  4.9360e-02],\n",
      "          [ 8.7752e-02,  1.6453e-02]],\n",
      "\n",
      "         [[ 1.2614e-01,  3.8391e-02],\n",
      "          [ 4.3876e-02, -5.4845e-03]],\n",
      "\n",
      "         [[ 2.0293e-01,  4.1134e-01],\n",
      "          [ 7.6783e-02,  2.7971e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 6.8008e-01,  8.0622e-01],\n",
      "          [ 2.9068e-01,  2.9068e-01]],\n",
      "\n",
      "         [[ 1.9744e-01,  2.9068e-01],\n",
      "          [-3.2358e-01, -1.9196e-01]],\n",
      "\n",
      "         [[ 3.2907e-02, -8.2267e-02],\n",
      "          [ 8.2267e-02,  7.1298e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.0585e-01,  1.0421e-01],\n",
      "          [-7.1298e-02, -1.5905e-01]],\n",
      "\n",
      "         [[ 2.4132e-01, -6.0329e-02],\n",
      "          [-7.1298e-02, -3.8391e-02]],\n",
      "\n",
      "         [[ 3.8391e-01,  6.0878e-01],\n",
      "          [ 1.3711e-01,  3.7843e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.8721e-02, -1.0421e-01],\n",
      "          [ 1.7002e-01,  2.6326e-01]],\n",
      "\n",
      "         [[ 2.8519e-01,  2.3583e-01],\n",
      "          [ 5.4845e-03,  3.2907e-02]],\n",
      "\n",
      "         [[ 2.9616e-01,  5.4845e-03],\n",
      "          [ 2.1390e-01,  1.7186e-08]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.5814e-02, -1.2614e-01],\n",
      "          [ 2.2486e-01, -2.1938e-02]],\n",
      "\n",
      "         [[ 1.5357e-01,  3.6746e-01],\n",
      "          [ 5.1006e-01,  2.8519e-01]],\n",
      "\n",
      "         [[-1.2066e-01, -7.1298e-02],\n",
      "          [ 2.7971e-01,  1.7550e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.7752e-02, -1.6453e-02],\n",
      "          [-2.3583e-01, -3.0713e-01]],\n",
      "\n",
      "         [[ 7.6783e-02, -7.1298e-02],\n",
      "          [ 1.2614e-01,  2.1938e-02]],\n",
      "\n",
      "         [[ 1.4260e-01, -1.6453e-02],\n",
      "          [ 2.2486e-01, -5.4845e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.2907e-02,  3.8391e-02],\n",
      "          [-1.3711e-01, -6.5814e-02]],\n",
      "\n",
      "         [[ 2.8519e-01,  2.6874e-01],\n",
      "          [ 3.8940e-01,  4.2779e-01]],\n",
      "\n",
      "         [[ 2.6326e-01,  1.7002e-01],\n",
      "          [ 8.2267e-02,  2.0841e-01]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "output_int =  conv_int(x_int)    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int * x_delta * w_delta # recover with x_delta and w_delta\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0281, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, bias = False)\n",
    "weight = model.features[27].weight_q\n",
    "\n",
    "conv_ref.weight = weight\n",
    "conv_ref.bias = model.features[3].bias\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(abs((output_ref - output_recovered)).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 4])\n",
      "torch.Size([8, 8, 9])\n",
      "range(0, 4)\n",
      "range(0, 4)\n",
      "range(0, 8)\n",
      "range(0, 8)\n",
      "range(0, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a_int = x_int[0,:,:,:]  # pick only one input out of batch\n",
    "# a_int.size() = [8, 4, 4]\n",
    "print(a_int.size())\n",
    "w_int = torch.reshape(weight_int, (weight_int.size(0), weight_int.size(1), -1))  # merge ki, kj index to kij\n",
    "# w_int.weight.size() = torch.Size([8, 8, 9])\n",
    "print(w_int.size())\n",
    "padding = 1\n",
    "stride = 1\n",
    "array_size = 8 # row and column number\n",
    "\n",
    "nig = range(a_int.size(1))  ## ni group\n",
    "njg = range(a_int.size(2))  ## nj group\n",
    "print(nig)\n",
    "print(njg)\n",
    "icg = range(int(w_int.size(1)))  ## input channel \n",
    "ocg = range(int(w_int.size(0)))  ## output channel\n",
    "print(icg)\n",
    "print(ocg)\n",
    "ic_tileg = range(int(len(icg)/array_size))\n",
    "oc_tileg = range(int(len(ocg)/array_size))\n",
    "#print(ic_tileg)\n",
    "#print(oc_tileg)\n",
    "kijg = range(w_int.size(2))\n",
    "ki_dim = int(math.sqrt(w_int.size(2)))  ## Kernel's 1 dim size\n",
    "print(kijg)\n",
    "\n",
    "######## Padding before Convolution #######\n",
    "a_pad = torch.zeros(len(icg), len(nig)+padding*2, len(nig)+padding*2).cuda()\n",
    "# a_pad.size() = [8, 4+2pad, 4+2pad]\n",
    "a_pad[ :, padding:padding+len(nig), padding:padding+len(njg)] = a_int.cuda()\n",
    "a_pad = torch.reshape(a_pad, (a_pad.size(0), -1))\n",
    "# a_pad.size() = [8, (4+2pad)*(4+2pad)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66d87ae9-1a52-4fe1-938c-244b972ca8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range(0, 36)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_nijg = range(a_pad.size(1)) ## paded activation's nij group [0, ...34*34-1]\n",
    "print(p_nijg)\n",
    "psum = torch.zeros( array_size, len(p_nijg), len(kijg)).cuda() \n",
    "\n",
    "for kij in kijg:       \n",
    "    for nij in p_nijg:     # time domain, sequentially given input\n",
    "        m = nn.Linear(array_size, array_size, bias=False)\n",
    "        m.weight = torch.nn.Parameter(w_int[:,:,kij])\n",
    "        psum[:, nij, kij] = m(a_pad[:,nij]).cuda()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa572583-076d-4352-8100-7117b3302346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "a_pad_ni_dim = int(math.sqrt(a_pad.size(1))) # 32 + 2*pad = 34\n",
    "\n",
    "o_ni_dim = int((a_pad_ni_dim - (ki_dim- 1) - 1)/stride + 1) #34 - 2 - 1 + 1 = 32\n",
    "o_nijg = range(o_ni_dim**2) # [0, 32*32-1]    \n",
    "    \n",
    "out = torch.zeros(len(ocg), len(o_nijg)).cuda()\n",
    "  \n",
    "   \n",
    "### SFP accumulation ###\n",
    "for o_nij in o_nijg: \n",
    "    for kij in kijg:  #[0, ... 8]\n",
    "        out[:,o_nij] = out[:,o_nij] + \\\n",
    "        psum[:, int(o_nij/o_ni_dim)*a_pad_ni_dim + o_nij%o_ni_dim + int(kij/ki_dim)*a_pad_ni_dim + kij%ki_dim, kij]\n",
    "                ## 2nd index = (int(o_nij/30)*32 + o_nij%30) + (int(kij/3)*32 + kij%3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  4.0000, 11.0000,  9.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.0000,  3.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  2.0000,  3.0000,  3.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.0000,  1.0000]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "### show this cell partially. The following cells should be printed by students ###\n",
    "nij = 0 # just a random number\n",
    "X = a_pad[:,nij:nij+8]  # [tile_num, array row num, time_steps]\n",
    "print(X)\n",
    "bit_precision = 4\n",
    "file = open('activation.txt', 'w') #write to file\n",
    "file.write('#time0row7[msb-lsb],time0row6[msb-lst],....,time0row0[msb-lst]#\\n')\n",
    "file.write('#time1row7[msb-lsb],time1row6[msb-lst],....,time1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(X.size(1)):  # time step\n",
    "    for j in range(X.size(0)): # row #\n",
    "        X_bin = '{0:04b}'.format(round(X[7-j,i].item()))\n",
    "        for k in range(bit_precision):\n",
    "            file.write(X_bin[k])        \n",
    "        #file.write(' ')  # for visibility with blank between words, you can use\n",
    "    file.write('\\n')\n",
    "file.close() #close file    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "kij = 8\n",
    "W = w_int[:,:,kij]  # w_tile[tile_num, array col num, array row num, kij]\n",
    "\n",
    "\n",
    "bit_precision = 4\n",
    "file = open('weight8.txt', 'w') #write to file\n",
    "file.write('#col0row7[msb-lsb],col0row6[msb-lst],....,col0row0[msb-lst]#\\n')\n",
    "file.write('#col1row7[msb-lsb],col1row6[msb-lst],....,col1row0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "\n",
    "for i in range(W.size(1)):  # time step\n",
    "    for j in range(W.size(0)):  # row #\n",
    "        value = round(W[7-j, i].item())  # Get the value\n",
    "        if value < 0:\n",
    "            value = (1 << bit_precision) + value  # Two's complement for negatives\n",
    "        W_bin = '{0:04b}'.format(value)  # Format as 4-bit binary\n",
    "        for k in range(bit_precision):\n",
    "            file.write(W_bin[k])        \n",
    "    file.write('\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Complete this cell ###\n",
    "kij = 0\n",
    "nij = 2\n",
    "\n",
    "psum_tile = psum[:,nij:nij+8,kij]  \n",
    "# psum[len(ic_tileg), len(oc_tileg), array_size, len(p_nijg), len(kijg)]\n",
    "bit_precision = 16\n",
    "file = open('psum.txt', 'w') #write to file\n",
    "file.write('#time0col7[msb-lsb],time0col6[msb-lst],....,time0col0[msb-lst]#\\n')\n",
    "file.write('#time1col7[msb-lsb],time1col6[msb-lst],....,time1col0[msb-lst]#\\n')\n",
    "file.write('#................#\\n')\n",
    "\n",
    "for i in range(psum_tile.size(1)):  # time step\n",
    "    for j in range(psum_tile.size(0)):  # row #\n",
    "        value = round(psum_tile[7-j, i].item())  # Get the value\n",
    "        if value < 0:\n",
    "            value = (1 << bit_precision) + value  # Two's complement for negatives\n",
    "        psum_bin = '{0:016b}'.format(value)  # Format as 4-bit binary\n",
    "        for k in range(bit_precision):\n",
    "            file.write(psum_bin[k])        \n",
    "    file.write('\\n')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7dfd6-945d-430f-b53e-5383655887be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
